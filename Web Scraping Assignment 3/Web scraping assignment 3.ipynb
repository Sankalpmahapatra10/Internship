{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280a586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\sanka\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from selenium) (1.26.10)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (22.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (37.0.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\sanka\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d99c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product : Guitar\n"
     ]
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# enter the product that you want to search in amazon\n",
    "u_input = input('Enter the product : ')\n",
    "\n",
    "\n",
    "\n",
    "#finding web elemet for search bar and writing on search-job bar \n",
    "search_bar=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_bar.send_keys(u_input)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element(By.XPATH, \"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "search_btn.click()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766753fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6105af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product : guiter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# enter the product that you want to search in amazon\n",
    "u_input = input('Enter the product : ')\n",
    "\n",
    "\n",
    "\n",
    "#finding web elemet for search bar and writing on search-job bar \n",
    "search_bar=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_bar.send_keys(u_input)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element(By.XPATH, \"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "search_btn.click()\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# fetching all the urls in the 3 pages\n",
    "\n",
    "\n",
    "urls = []          \n",
    "for i in range(0,3):      \n",
    "    page_url = driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for i in page_url:\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "        next_btn = driver.find_element_by_xpath(\"//li[@class='a-last']/a\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        \n",
    "len(urls)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89d16186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c7110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 50 images\n",
      "Downloading 1 of 50 images\n",
      "Downloading 2 of 50 images\n",
      "Downloading 3 of 50 images\n",
      "Downloading 4 of 50 images\n",
      "Downloading 5 of 50 images\n",
      "Downloading 6 of 50 images\n",
      "Downloading 7 of 50 images\n",
      "Downloading 8 of 50 images\n",
      "Downloading 9 of 50 images\n",
      "Downloading 10 of 50 images\n",
      "Downloading 11 of 50 images\n",
      "Downloading 12 of 50 images\n",
      "Downloading 13 of 50 images\n",
      "Downloading 14 of 50 images\n",
      "Downloading 15 of 50 images\n",
      "Downloading 16 of 50 images\n",
      "Downloading 17 of 50 images\n",
      "Downloading 18 of 50 images\n",
      "Downloading 19 of 50 images\n",
      "Downloading 20 of 50 images\n",
      "Downloading 21 of 50 images\n",
      "Downloading 22 of 50 images\n",
      "Downloading 23 of 50 images\n",
      "Downloading 24 of 50 images\n",
      "Downloading 25 of 50 images\n",
      "Downloading 26 of 50 images\n",
      "Downloading 27 of 50 images\n",
      "Downloading 28 of 50 images\n",
      "Downloading 29 of 50 images\n",
      "Downloading 30 of 50 images\n",
      "Downloading 31 of 50 images\n",
      "Downloading 32 of 50 images\n",
      "Downloading 33 of 50 images\n",
      "Downloading 34 of 50 images\n",
      "Downloading 35 of 50 images\n",
      "Downloading 36 of 50 images\n",
      "Downloading 37 of 50 images\n",
      "Downloading 38 of 50 images\n",
      "Downloading 39 of 50 images\n",
      "Downloading 40 of 50 images\n",
      "Downloading 41 of 50 images\n",
      "Downloading 42 of 50 images\n",
      "Downloading 43 of 50 images\n",
      "Downloading 44 of 50 images\n",
      "Downloading 45 of 50 images\n",
      "Downloading 46 of 50 images\n",
      "Downloading 47 of 50 images\n",
      "Downloading 48 of 50 images\n",
      "Downloading 49 of 50 images\n"
     ]
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "\n",
    "# connecting to the url of our requirement\n",
    "url = \"https://www.google.com/\"\n",
    "\n",
    "#creating a empty list to store the urls\n",
    "urls = []\n",
    "\n",
    "\n",
    "search_items = [\"Fruits\",\"Cars\",\"Machine Learning\", \"Guiters\",\"cakes\"]\n",
    "for x in search_items:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # finding webelement for search_bar\n",
    "    search_bar = driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input\")\n",
    "    \n",
    "    # writing on the search bar\n",
    "    search_bar.send_keys(str(x))\n",
    "    \n",
    "    # clicking on search button\n",
    "    search_button = driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[2]/div[2]/div[5]/center/input[1]\")\n",
    "    time.sleep(3)\n",
    "    search_button.click()\n",
    "    \n",
    "    #selection of image tab in google\n",
    "    img_tab=driver.find_element(By.XPATH,\"/html/body/div[7]/div/div[4]/div/div[1]/div/div[1]/div/div[2]/a\")\n",
    "    time.sleep(3)\n",
    "    img_tab.click()\n",
    "    # scroling down the webpage to get some more images\n",
    "    for _ in range(30):\n",
    "        driver.execute_script(\"window.scrollBy(0,50)\")\n",
    "        \n",
    "        image = driver.find_elements(By.XPATH,\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_urls = []\n",
    "    for i in image:\n",
    "        source = i.get_attribute('src')\n",
    "        if source is not None:\n",
    "            if(source[0:4] == 'http'):\n",
    "                img_urls.append(source)\n",
    "    for i in img_urls[:50]:\n",
    "        urls.append(i)\n",
    "        \n",
    "for i in range(len(urls)):\n",
    "    if i >= 50:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i,50))\n",
    "    response = requests.get(urls[i])\n",
    "    file = open(r\"C:\\New folder\\imp\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f73f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9001cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "# opening our required url\n",
    "url = driver.get('https://www.flipkart.com/')\n",
    "time.sleep(5)\n",
    "# closing login popup button\n",
    "lonin_x_btn = driver.find_element_by_xpath(\"//div[@class='_2QfC02']//button\").click()\n",
    "# search for web element\n",
    "search_bar = driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "\n",
    "# sending keys to search product\n",
    "search_bar.send_keys(\"Iphone 11\")\n",
    "# location the search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "\n",
    "# clicking on search button\n",
    "search_btn.click()\n",
    "# fetching 1st page of URLs of smartphone\n",
    "page1_url = []\n",
    "urls = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "for url in urls:\n",
    "    page1_url.append(url.get_attribute('href'))\n",
    "len(page1_url)\n",
    "\n",
    "# creating empty list\n",
    "Smartphones = ({})\n",
    "Smartphones['Brand'] = []\n",
    "Smartphones['Phone name'] = []\n",
    "Smartphones['Colour'] = []\n",
    "Smartphones['RAM'] = []\n",
    "Smartphones['Storage(ROM)'] = []\n",
    "Smartphones['Primary Camera'] = []\n",
    "Smartphones['Display Size'] = []\n",
    "Smartphones['Display Resolution'] = []\n",
    "Smartphones['Processor'] = []\n",
    "Smartphones['Battery Capacity'] = []\n",
    "Smartphones['Price'] = []\n",
    "Smartphones['URL'] = []\n",
    "\n",
    "# scraping data from each url of page 1\n",
    "for url in page1_url:\n",
    "    driver.get(url)\n",
    "    print(\"Scraping URL = \",url)\n",
    "    Smartphones['URL'].append(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #clicking on read more button to get more information\n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\")\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "    \n",
    "    #scraping brand name of smartphone\n",
    "    try:\n",
    "        brand_tags = driver.find_element_by_xpath(\"//span[@class='B_NuCI']\")\n",
    "        Smartphones['Brand'].append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Brand'].append('-')\n",
    "    \n",
    "    \n",
    "    # scraping name of smartphones\n",
    "    try:\n",
    "        name_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][1]/table/tbody/tr[3]/td[2]/ul/li\")\n",
    "        Smartphones['Phone name'].append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Phone name'].append('-')\n",
    "        \n",
    "    #scraping colour of smartphone\n",
    "    try:\n",
    "        color_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][1]/table/tbody/tr[4]/td[2]/ul/li\")\n",
    "        Smartphones['Colour'].append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Colour'].append('-')\n",
    "        \n",
    "     # scraping RAM data of smartphone\n",
    "    try:\n",
    "        ram_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][4]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['RAM'].append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['RAM'].append('-')\n",
    "        \n",
    "    #scraping ROM data of smartphones\n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][4]/table[1]/tbody/tr[1]/td[2]/ul/li\")\n",
    "        Smartphones['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Storage(ROM)'].append('-')\n",
    "        \n",
    "    # scraping  Primary camera data of smartphone\n",
    "    try:\n",
    "        pri =driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Primary Camera'].append(pri.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Primary Camera'].append('-')\n",
    "        \n",
    "    \n",
    "     #scraping display size data of smartphone\n",
    "    try:\n",
    "        disp = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/div\")\n",
    "        if disp.text != 'Display Features' : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/table[1]/tbody/tr[1]/td[2]/ul/li\")\n",
    "        Smartphones['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Size'].append('-')\n",
    "        \n",
    "    \n",
    "    #scraping display resolution of smartphone\n",
    "    try:\n",
    "        disp = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/div\")\n",
    "        if disp.text != 'Display Features' : raise NoSuchElementException\n",
    "        disp_reso = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Display Resolution'].append(disp_reso.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Resolution'].append('-')\n",
    "        \n",
    "        \n",
    "    #scraping processor of smartphone\n",
    "    try:\n",
    "        pro = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[2]/td[1]]\")\n",
    "        if pro.text != 'Processor Type' : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor'].append('-')  \n",
    "        \n",
    "    \n",
    "   \n",
    "        \n",
    "    # scraping price of smartphone\n",
    "    try:\n",
    "        price_tags = driver.find_element_by_xpath(\"//div[@class='_30jeq3 _16Jk6d']\")\n",
    "        Smartphones['Price'].append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "          Smartphones['Price'].append('-')    \n",
    "            \n",
    "            \n",
    "\n",
    "#converting the scrapped ddata into a data frame\n",
    "\n",
    "df = pd.DataFrame.from_dict(Smartphones)\n",
    "df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0bd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter City name that has to be searched : bhubaneswar\n",
      "bhubaneswar  Longitude:  23.5616256 \"N\n",
      "bhubaneswar  Lattitude:  87.311552 \"E\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "# opening our required url\n",
    "url = driver.get('https://www.google.co.in/maps/@23.5616256,87.311552,15z')\n",
    "time.sleep(5)\n",
    "\n",
    "# entering the city name in search bar\n",
    "city_name = input('Enter City name that has to be searched : ')\n",
    "search_bar = driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/form/div[2]/div[3]/div/input[1]')\n",
    "time.sleep(2)\n",
    "\n",
    "#writing on the search bar\n",
    "search_bar.send_keys(city_name)\n",
    "\n",
    "#clicking on the search button\n",
    "search_btn = driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/form/div[2]/div[3]/div/input[1]\")\n",
    "search_btn.click()\n",
    "time.sleep(5)\n",
    "\n",
    "current_url=driver.current_url\n",
    "longitude=current_url.split('@')[1].split(',')[0]\n",
    "lattitude=current_url.split('@')[1].split(',')[1]\n",
    "print(city_name ,\" Longitude: \",longitude,'\"N')\n",
    "print(city_name ,\" Lattitude: \",lattitude,'\"E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd681fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851de72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217ca32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup Name</th>\n",
       "      <th>Industry/Vertical</th>\n",
       "      <th>Sub_Vertical</th>\n",
       "      <th>Location</th>\n",
       "      <th>Investor</th>\n",
       "      <th>Investment Type</th>\n",
       "      <th>Amount(in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/01/2021</td>\n",
       "      <td>Digit Insurance</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Insurance Services</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>A91 Partners, Faering Capital, TVS Capital Funds</td>\n",
       "      <td>Venture</td>\n",
       "      <td>1,80,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28/01/2021</td>\n",
       "      <td>Bombay Shaving Company</td>\n",
       "      <td>Consumer Goods Company</td>\n",
       "      <td>Shave care, beard care, and skincare products</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Reckitt Benckiser</td>\n",
       "      <td>Venture</td>\n",
       "      <td>6,172,258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>DeHaat</td>\n",
       "      <td>AgriTech Startup</td>\n",
       "      <td>online marketplace for farm products and services</td>\n",
       "      <td>Patna</td>\n",
       "      <td>Prosus Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>Darwinbox</td>\n",
       "      <td>SaaS</td>\n",
       "      <td>HR Tech</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Salesforce Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>mfine</td>\n",
       "      <td>Health Tech Startup</td>\n",
       "      <td>AI-powered telemedicine mobile app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Heritas Capital Management</td>\n",
       "      <td>Venture Round</td>\n",
       "      <td>16,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>Udayy</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Online learning platform for kids in class 1-5</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Sequoia Capital</td>\n",
       "      <td>Seed Funding</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11/01/2021</td>\n",
       "      <td>True Elements</td>\n",
       "      <td>Food Startup</td>\n",
       "      <td>Whole Food plant based Nashta</td>\n",
       "      <td>Pune</td>\n",
       "      <td>SIDBI Venture Capital</td>\n",
       "      <td>Series</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13/01/2021</td>\n",
       "      <td>Saveo</td>\n",
       "      <td>B2B E-commerce</td>\n",
       "      <td>Pharmacies</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Matrix Partners India, RTP Global, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11/02/2021</td>\n",
       "      <td>Doubtnut</td>\n",
       "      <td>Edu Tech</td>\n",
       "      <td>E-Learning Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SIG Global, Sequoia Capital, WaterBridge Ventu...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>2,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22/02/2021</td>\n",
       "      <td>Zomato</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Online Food Delivery Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Tiger Global, Kora</td>\n",
       "      <td>Venture</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19/02/2021</td>\n",
       "      <td>Fingerlix</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Semi-cooked food delivery app</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Rhodium Trust, Accel Partners and Swiggy</td>\n",
       "      <td>Series C</td>\n",
       "      <td>2,747,045.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17/02/2021</td>\n",
       "      <td>Zolve</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Global Neobank Venture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Accel Partners and Lightspeed Venture Partners</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,50,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15/02/2021</td>\n",
       "      <td>KreditBee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Digital lending platform</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Azim Premji’s PremjiInvest and South Korea’s M...</td>\n",
       "      <td>Series C</td>\n",
       "      <td>75,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Pepperfry</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Multi-brand furniture brand</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>InnoVen Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>4,773,958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Online supermarket</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SoftBank Vision Fund (SVF)</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>55,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Technology Venture</td>\n",
       "      <td>London</td>\n",
       "      <td>GV</td>\n",
       "      <td>Series A</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>SplashLearn</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Game-based learning programme</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Owl Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>18,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>04/03/2021</td>\n",
       "      <td>DealShare</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online shopping platform</td>\n",
       "      <td>Jaipur, Rajasthan</td>\n",
       "      <td>Innoven Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31/03/2021</td>\n",
       "      <td>Uniphore</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Conversational Service Automation (CSA)</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Sorenson Capital Partners</td>\n",
       "      <td>Series D</td>\n",
       "      <td>140,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Hyper-local delivery app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Krishtal Advisors Pte Ltd</td>\n",
       "      <td>Series E</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>BYJU’S</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Online tutoring</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>MC Global Edtech, B Capital, Baron, others</td>\n",
       "      <td>Series F</td>\n",
       "      <td>460,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23/03/2021</td>\n",
       "      <td>SkilloVilla</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Career and job-oriented upskilling.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Titan Capital, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>300,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>CityMall</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Social ecommerce and online grocery platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Accel Partners</td>\n",
       "      <td>Series A</td>\n",
       "      <td>11,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>DotPe</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Commerce and payments platform to offline ente...</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Series A</td>\n",
       "      <td>27,500,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date            Startup Name       Industry/Vertical  \\\n",
       "0   15/01/2021         Digit Insurance      Financial Services   \n",
       "1   28/01/2021  Bombay Shaving Company  Consumer Goods Company   \n",
       "2   19/01/2021                  DeHaat        AgriTech Startup   \n",
       "3   19/01/2021               Darwinbox                    SaaS   \n",
       "4   18/01/2021                   mfine     Health Tech Startup   \n",
       "5   18/01/2021                   Udayy                  EdTech   \n",
       "6   11/01/2021           True Elements            Food Startup   \n",
       "7   13/01/2021                   Saveo          B2B E-commerce   \n",
       "8   11/02/2021                Doubtnut                Edu Tech   \n",
       "9   22/02/2021                  Zomato             Hospitality   \n",
       "10  19/02/2021               Fingerlix             Hospitality   \n",
       "11  17/02/2021                   Zolve                 FinTech   \n",
       "12  15/02/2021               KreditBee                 Finance   \n",
       "13  12/02/2021               Pepperfry              E-commerce   \n",
       "14  12/02/2021                 Grofers              E-Commerce   \n",
       "15  09/02/2021                 Nothing              Technology   \n",
       "16  09/02/2021             SplashLearn                  EdTech   \n",
       "17  04/03/2021               DealShare              E-commerce   \n",
       "18  31/03/2021                Uniphore              Technology   \n",
       "19  30/03/2021                   Dunzo              E-commerce   \n",
       "20  30/03/2021                  BYJU’S                Edu-tech   \n",
       "21  23/03/2021             SkilloVilla                Edu-tech   \n",
       "22  25/03/2021                CityMall              E-commerce   \n",
       "23  26/03/2021                   DotPe                 FinTech   \n",
       "\n",
       "                                         Sub_Vertical           Location  \\\n",
       "0                                  Insurance Services          Bengaluru   \n",
       "1       Shave care, beard care, and skincare products          New Delhi   \n",
       "2   online marketplace for farm products and services              Patna   \n",
       "3                                             HR Tech             Mumbai   \n",
       "4                  AI-powered telemedicine mobile app          Bengaluru   \n",
       "5      Online learning platform for kids in class 1-5            Gurgaon   \n",
       "6                       Whole Food plant based Nashta               Pune   \n",
       "7                                          Pharmacies          Bengaluru   \n",
       "8                                 E-Learning Platform            Gurgaon   \n",
       "9                       Online Food Delivery Platform            Gurgaon   \n",
       "10                      Semi-cooked food delivery app             Mumbai   \n",
       "11                             Global Neobank Venture             Mumbai   \n",
       "12                           Digital lending platform          Bengaluru   \n",
       "13                        Multi-brand furniture brand             Mumbai   \n",
       "14                                 Online supermarket            Gurgaon   \n",
       "15                        Consumer Technology Venture             London   \n",
       "16                      Game-based learning programme            Gurgaon   \n",
       "17                           Online shopping platform  Jaipur, Rajasthan   \n",
       "18            Conversational Service Automation (CSA)          Palo Alto   \n",
       "19                           Hyper-local delivery app          Bengaluru   \n",
       "20                                    Online tutoring          Bengaluru   \n",
       "21                Career and job-oriented upskilling.          Bengaluru   \n",
       "22       Social ecommerce and online grocery platform            Gurgaon   \n",
       "23  Commerce and payments platform to offline ente...            Gurgaon   \n",
       "\n",
       "                                             Investor Investment Type  \\\n",
       "0    A91 Partners, Faering Capital, TVS Capital Funds         Venture   \n",
       "1                                   Reckitt Benckiser         Venture   \n",
       "2                                     Prosus Ventures        Series C   \n",
       "3                                 Salesforce Ventures            Seed   \n",
       "4                          Heritas Capital Management   Venture Round   \n",
       "5                                     Sequoia Capital    Seed Funding   \n",
       "6                               SIDBI Venture Capital          Series   \n",
       "7           Matrix Partners India, RTP Global, others            Seed   \n",
       "8   SIG Global, Sequoia Capital, WaterBridge Ventu...        Series B   \n",
       "9                                  Tiger Global, Kora         Venture   \n",
       "10           Rhodium Trust, Accel Partners and Swiggy        Series C   \n",
       "11     Accel Partners and Lightspeed Venture Partners            Seed   \n",
       "12  Azim Premji’s PremjiInvest and South Korea’s M...        Series C   \n",
       "13                                    InnoVen Capital  Debt Financing   \n",
       "14                         SoftBank Vision Fund (SVF)     Unspecified   \n",
       "15                                                 GV        Series A   \n",
       "16                                       Owl Ventures        Series C   \n",
       "17                                    Innoven Capital  Debt Financing   \n",
       "18                          Sorenson Capital Partners        Series D   \n",
       "19                          Krishtal Advisors Pte Ltd        Series E   \n",
       "20         MC Global Edtech, B Capital, Baron, others        Series F   \n",
       "21                              Titan Capital, others            Seed   \n",
       "22                                     Accel Partners        Series A   \n",
       "23                                               PayU        Series A   \n",
       "\n",
       "   Amount(in USD)  \n",
       "0     1,80,00,000  \n",
       "1    6,172,258.50  \n",
       "2      30,000,000  \n",
       "3      15,000,000  \n",
       "4      16,000,000  \n",
       "5      15,000,000  \n",
       "6     100,000,000  \n",
       "7       4,000,000  \n",
       "8       2,500,000  \n",
       "9     250,000,000  \n",
       "10   2,747,045.20  \n",
       "11    1,50,00,000  \n",
       "12     75,000,000  \n",
       "13      4,773,958  \n",
       "14     55,000,000  \n",
       "15     15,000,000  \n",
       "16     18,000,000  \n",
       "17    250,000,000  \n",
       "18    140,000,000  \n",
       "19      8,000,000  \n",
       "20    460,000,000  \n",
       "21    300,000,000  \n",
       "22     11,000,000  \n",
       "23     27,500,000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://trak.in/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# getting xpath for funding deals tab and scraping the href link\n",
    "funding_tab = driver.find_element(By.XPATH,\"//li[@id='menu-item-51510']/a\").get_attribute('href')\n",
    "driver.get(funding_tab)\n",
    "\n",
    "#creating Empty Lists for storing different information\n",
    "deals = {}\n",
    "deals['Date'] = []\n",
    "deals['Startup Name'] = []\n",
    "deals['Industry/Vertical'] = []\n",
    "deals['Sub_Vertical'] = []\n",
    "deals['Location'] = []\n",
    "deals['Investor'] = []\n",
    "deals['Investment Type'] = []\n",
    "deals['Amount(in USD)'] = []\n",
    "\n",
    "\n",
    "for i in range(54,57):\n",
    "    \n",
    "    # scraping date\n",
    "    date = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[2]\".format(i))\n",
    "    for d in date:\n",
    "        deals['Date'].append(d.text)\n",
    "        \n",
    "    # scraping startup name\n",
    "    startup_name = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[3]\".format(i))\n",
    "    for name in startup_name:\n",
    "        deals['Startup Name'].append(name.text)\n",
    "        \n",
    "    \n",
    "    #scraping industry or vertical\n",
    "    industry = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[4]\".format(i))\n",
    "    for ind in industry:\n",
    "        deals['Industry/Vertical'].append(ind.text)\n",
    "    \n",
    "     #scraping sub-vertical\n",
    "    sub_vertical = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[5]\".format(i))\n",
    "    for sv in sub_vertical:\n",
    "        deals['Sub_Vertical'].append(sv.text)\n",
    "        \n",
    "        \n",
    "    # scraping location\n",
    "    location = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[6]\".format(i))\n",
    "    for loc in location:\n",
    "        deals['Location'].append(loc.text)\n",
    "        \n",
    "        \n",
    "    # scraping investor\n",
    "    investor = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[7]\".format(i))\n",
    "    for invest in investor:\n",
    "        deals['Investor'].append(invest.text)\n",
    "        \n",
    "        \n",
    "    # scraping investment type\n",
    "    investment_type = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[8]\".format(i))\n",
    "    for invtype in investment_type:\n",
    "        deals['Investment Type'].append(invtype.text)\n",
    "        \n",
    "        \n",
    "    # scraping amount\n",
    "    amount = driver.find_elements(By.XPATH,\"//table[@id='tablepress-{}']/tbody/tr/td[9]\".format(i))\n",
    "    for amt in amount:\n",
    "        deals['Amount(in USD)'].append(amt.text)\n",
    "        \n",
    "\n",
    "\n",
    "# creating DataFrame for scraped data\n",
    "data= pd.DataFrame(deals)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43f6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d334025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Name</th>\n",
       "      <th>Operating System</th>\n",
       "      <th>Display</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Graphical Processor</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSI TITAN GT77-12UHS</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>17.3\" (3840 X 2160) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>64 GB DDR5 RAM &amp; 2 TB SSD</td>\n",
       "      <td>16 GB DDR6 NVIDIA GeForce RTX 3080 Ti Graphics...</td>\n",
       "      <td>397 x 330 x 23 mm dimension &amp; 3.3 kg weight</td>\n",
       "      <td>64 GB DDR5 RAM &amp; 2 TB SSD</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALIENWARE X17 R2</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>17.3\" (1920X1080) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>32 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>16 GB DDR6 NVIDIA GeForce RTX 3080 Ti, Graphic...</td>\n",
       "      <td>399.23 x 299.57 x 14.75 mm dimension &amp; 3.02 kg...</td>\n",
       "      <td>32 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>Rs389,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACER PREDATOR TRITON 500 SE PT516-52S</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16\" (2560 X 1600) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>32 GB DDR5 RAM &amp; 2 TB SSD</td>\n",
       "      <td>8 GB DDR6 NVIDIA GeForce RTXTM 3070 Ti Graphic...</td>\n",
       "      <td>358 x 262 x 19.9 mm dimension &amp; 2.4 kg weight</td>\n",
       "      <td>32 GB DDR5 RAM &amp; 2 TB SSD</td>\n",
       "      <td>Rs300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OMEN BY HP (16-B1371TX)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16.1\" (2560 X 1440) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>8 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>8 GB GDDR6 NVIDIA GeForce RTX 3070 Graphics card</td>\n",
       "      <td>369 x 248 x 23 mm dimension &amp; 2.32 kg weight</td>\n",
       "      <td>8 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>Rs195,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACER PREDATOR HELIOS 300 AN515-45 (NH.QBRSI.0</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6\" (2560 X 1440) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 512 GB SSD</td>\n",
       "      <td>8 GB DDR6 NVIDIA GeForce RTX 3070 Graphics card</td>\n",
       "      <td>363 x 255 x 23.9 mm dimension &amp; 2.4 kg weight</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 512 GB SSD</td>\n",
       "      <td>Rs172,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MSI DELTA 15 (A5EFK-083IN)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6\" (1920 X 1080) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB SSD</td>\n",
       "      <td>10 GB DDR6 AMD Radeon RX 6700M Graphics card</td>\n",
       "      <td>357 x 247 x 19 mm dimension &amp; 1.9 kg weight</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB SSD</td>\n",
       "      <td>Rs188,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OMEN BY HP (16-C0141AX)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16.1\" (2560 X 1440) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB NVMe</td>\n",
       "      <td>8 GB GDDR6 AMD Radeon™ RX 6600M Graphics card</td>\n",
       "      <td>36.92 x 24.8 x 2.3 mm dimension &amp; 2.3 kg weight</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB NVMe</td>\n",
       "      <td>Rs129,899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LENOVO LEGION 5I PRO (82RF00MGIN)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16\" (2560 X 1600) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card</td>\n",
       "      <td>359.9 x 264.4 x 19.9 mm dimension &amp; 2.5 kg weight</td>\n",
       "      <td>16 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>Rs230,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALIENWARE M15 R5 RYZEN EDITION ICC-C780001WIN</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6\" (1920 X 1080) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 512 GB SSD</td>\n",
       "      <td>6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card</td>\n",
       "      <td>356.2 x 272.5 x 22.85 mm dimension &amp; 2.69 kg w...</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 512 GB SSD</td>\n",
       "      <td>Rs140,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LENOVO SLIM 7 GEN 6 (82K8002JIN)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6 MP | NA DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB SSD</td>\n",
       "      <td>6 GB DDR6 NVIDIA GeForce 3060 Max-Q Graphics card</td>\n",
       "      <td>356 x 252 x 16 mm dimension &amp; 1.9 kg weight</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB SSD</td>\n",
       "      <td>Rs135,490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Laptop Name    Operating System  \\\n",
       "0                           MSI TITAN GT77-12UHS  WINDOWS 11 HOME OS   \n",
       "1                               ALIENWARE X17 R2  WINDOWS 11 HOME OS   \n",
       "2          ACER PREDATOR TRITON 500 SE PT516-52S  WINDOWS 11 HOME OS   \n",
       "3                        OMEN BY HP (16-B1371TX)  WINDOWS 11 HOME OS   \n",
       "4  ACER PREDATOR HELIOS 300 AN515-45 (NH.QBRSI.0  WINDOWS 11 HOME OS   \n",
       "5                     MSI DELTA 15 (A5EFK-083IN)  WINDOWS 11 HOME OS   \n",
       "6                        OMEN BY HP (16-C0141AX)  WINDOWS 11 HOME OS   \n",
       "7              LENOVO LEGION 5I PRO (82RF00MGIN)  WINDOWS 11 HOME OS   \n",
       "8  ALIENWARE M15 R5 RYZEN EDITION ICC-C780001WIN  WINDOWS 11 HOME OS   \n",
       "9               LENOVO SLIM 7 GEN 6 (82K8002JIN)  WINDOWS 11 HOME OS   \n",
       "\n",
       "                       Display        Processor                       Memory  \\\n",
       "0  17.3\" (3840 X 2160) DISPLAY  Windows 11 Home    64 GB DDR5 RAM & 2 TB SSD   \n",
       "1    17.3\" (1920X1080) DISPLAY  Windows 11 Home    32 GB DDR5 RAM & 1 TB SSD   \n",
       "2    16\" (2560 X 1600) DISPLAY  Windows 11 Home    32 GB DDR5 RAM & 2 TB SSD   \n",
       "3  16.1\" (2560 X 1440) DISPLAY  Windows 11 Home     8 GB DDR5 RAM & 1 TB SSD   \n",
       "4  15.6\" (2560 X 1440) DISPLAY  Windows 11 Home  16 GB DDR4 RAM & 512 GB SSD   \n",
       "5  15.6\" (1920 X 1080) DISPLAY  Windows 11 Home    16 GB DDR4 RAM & 1 TB SSD   \n",
       "6  16.1\" (2560 X 1440) DISPLAY  Windows 11 Home   16 GB DDR4 RAM & 1 TB NVMe   \n",
       "7    16\" (2560 X 1600) DISPLAY  Windows 11 Home    16 GB DDR5 RAM & 1 TB SSD   \n",
       "8  15.6\" (1920 X 1080) DISPLAY  Windows 11 Home  16 GB DDR4 RAM & 512 GB SSD   \n",
       "9         15.6 MP | NA DISPLAY  Windows 11 Home    16 GB DDR4 RAM & 1 TB SSD   \n",
       "\n",
       "                                              Weight  \\\n",
       "0  16 GB DDR6 NVIDIA GeForce RTX 3080 Ti Graphics...   \n",
       "1  16 GB DDR6 NVIDIA GeForce RTX 3080 Ti, Graphic...   \n",
       "2  8 GB DDR6 NVIDIA GeForce RTXTM 3070 Ti Graphic...   \n",
       "3   8 GB GDDR6 NVIDIA GeForce RTX 3070 Graphics card   \n",
       "4    8 GB DDR6 NVIDIA GeForce RTX 3070 Graphics card   \n",
       "5       10 GB DDR6 AMD Radeon RX 6700M Graphics card   \n",
       "6      8 GB GDDR6 AMD Radeon™ RX 6600M Graphics card   \n",
       "7    6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card   \n",
       "8    6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card   \n",
       "9  6 GB DDR6 NVIDIA GeForce 3060 Max-Q Graphics card   \n",
       "\n",
       "                                          Dimensions  \\\n",
       "0        397 x 330 x 23 mm dimension & 3.3 kg weight   \n",
       "1  399.23 x 299.57 x 14.75 mm dimension & 3.02 kg...   \n",
       "2      358 x 262 x 19.9 mm dimension & 2.4 kg weight   \n",
       "3       369 x 248 x 23 mm dimension & 2.32 kg weight   \n",
       "4      363 x 255 x 23.9 mm dimension & 2.4 kg weight   \n",
       "5        357 x 247 x 19 mm dimension & 1.9 kg weight   \n",
       "6    36.92 x 24.8 x 2.3 mm dimension & 2.3 kg weight   \n",
       "7  359.9 x 264.4 x 19.9 mm dimension & 2.5 kg weight   \n",
       "8  356.2 x 272.5 x 22.85 mm dimension & 2.69 kg w...   \n",
       "9        356 x 252 x 16 mm dimension & 1.9 kg weight   \n",
       "\n",
       "           Graphical Processor      Price  \n",
       "0    64 GB DDR5 RAM & 2 TB SSD        N/A  \n",
       "1    32 GB DDR5 RAM & 1 TB SSD  Rs389,990  \n",
       "2    32 GB DDR5 RAM & 2 TB SSD  Rs300,000  \n",
       "3     8 GB DDR5 RAM & 1 TB SSD  Rs195,600  \n",
       "4  16 GB DDR4 RAM & 512 GB SSD  Rs172,999  \n",
       "5    16 GB DDR4 RAM & 1 TB SSD  Rs188,990  \n",
       "6   16 GB DDR4 RAM & 1 TB NVMe  Rs129,899  \n",
       "7    16 GB DDR5 RAM & 1 TB SSD  Rs230,890  \n",
       "8  16 GB DDR4 RAM & 512 GB SSD  Rs140,999  \n",
       "9    16 GB DDR4 RAM & 1 TB SSD  Rs135,490  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.digit.in/\")\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# searching for best Laptop\n",
    "best_gaming_laptops = driver.find_element_by_xpath(\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty list for storing information\n",
    "Laptop_Name = []\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Weight = []\n",
    "Dimensions = []\n",
    "Graph_proc = []\n",
    "Price = []\n",
    "\n",
    "#scraping laptop names\n",
    "laptop_name = driver.find_elements(By.XPATH,\"//div[@class='right-container']/div/a/h3\")\n",
    "for name in laptop_name:\n",
    "    Laptop_Name.append(name.text)\n",
    "    \n",
    "#scraping operating system\n",
    "try:\n",
    "    op_sys = driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "    for os in op_sys:\n",
    "        Operating_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#scraping display type\n",
    "try:\n",
    "    display = driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "    for disp in display:\n",
    "        Display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping  processor type\n",
    "try:\n",
    "    processor = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[5]/td[3]\")\n",
    "    for pro in processor:\n",
    "        Processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping  memory details\n",
    "try:\n",
    "    memory = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for memo in memory:\n",
    "        Memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping weight of laptops\n",
    "try:\n",
    "    weight = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "    for wgt in weight:\n",
    "        Weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping laptop dimensions\n",
    "try:\n",
    "    dimension = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\")\n",
    "    for dim in dimension:\n",
    "        Dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping graphics processor details\n",
    "try:\n",
    "    graph = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for gra in graph:\n",
    "        Graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping price of the laptop\n",
    "try:\n",
    "    price = driver.find_elements(By.XPATH,\"//td[@class='smprice']\")\n",
    "    for pri in price:\n",
    "        Price.append(pri.text.replace('₹ ','Rs'))\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "#checking the length of all lists\n",
    "print(len(Laptop_Name),len(Operating_sys),len(Display),len(Processor),len(Memory),len(Weight),len(Dimensions),len(Graph_proc),len(Price))\n",
    "\n",
    "#creating DataFrame for scraped data\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "Gaming_Laptop['Laptop Name'] = Laptop_Name\n",
    "Gaming_Laptop['Operating System'] =Operating_sys\n",
    "Gaming_Laptop['Display'] = Display\n",
    "Gaming_Laptop['Processor'] = Processor\n",
    "Gaming_Laptop['Memory'] = Memory\n",
    "Gaming_Laptop['Weight'] = Weight\n",
    "Gaming_Laptop['Dimensions'] = Dimensions\n",
    "Gaming_Laptop['Graphical Processor'] = Graph_proc\n",
    "Gaming_Laptop['Price'] = Price\n",
    "Gaming_Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf36ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "215245c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 200 200 200 200 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net Worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$219 B</td>\n",
       "      <td>50</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$171 B</td>\n",
       "      <td>58</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$158 B</td>\n",
       "      <td>73</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$129 B</td>\n",
       "      <td>66</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Warren Buffett</td>\n",
       "      <td>$118 B</td>\n",
       "      <td>91</td>\n",
       "      <td>United States</td>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>192.</td>\n",
       "      <td>Marcel Herrmann Telles</td>\n",
       "      <td>$10.3 B</td>\n",
       "      <td>72</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>beer</td>\n",
       "      <td>Food &amp; Beverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197.</td>\n",
       "      <td>Leon Black</td>\n",
       "      <td>$10 B</td>\n",
       "      <td>70</td>\n",
       "      <td>United States</td>\n",
       "      <td>private equity</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197.</td>\n",
       "      <td>Joe Gebbia</td>\n",
       "      <td>$10 B</td>\n",
       "      <td>40</td>\n",
       "      <td>United States</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>197.</td>\n",
       "      <td>David Geffen</td>\n",
       "      <td>$10 B</td>\n",
       "      <td>79</td>\n",
       "      <td>United States</td>\n",
       "      <td>movies, record labels</td>\n",
       "      <td>Media &amp; Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>197.</td>\n",
       "      <td>Yu Renrong</td>\n",
       "      <td>$10 B</td>\n",
       "      <td>56</td>\n",
       "      <td>China</td>\n",
       "      <td>semiconductors</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                      Name Net Worth Age    Citizenship  \\\n",
       "0      1.                 Elon Musk    $219 B  50  United States   \n",
       "1      2.                Jeff Bezos    $171 B  58  United States   \n",
       "2      3.  Bernard Arnault & family    $158 B  73         France   \n",
       "3      4.                Bill Gates    $129 B  66  United States   \n",
       "4      5.            Warren Buffett    $118 B  91  United States   \n",
       "..    ...                       ...       ...  ..            ...   \n",
       "195  192.    Marcel Herrmann Telles   $10.3 B  72         Brazil   \n",
       "196  197.                Leon Black     $10 B  70  United States   \n",
       "197  197.                Joe Gebbia     $10 B  40  United States   \n",
       "198  197.              David Geffen     $10 B  79  United States   \n",
       "199  197.                Yu Renrong     $10 B  56          China   \n",
       "\n",
       "                    Source               Industry  \n",
       "0            Tesla, SpaceX             Automotive  \n",
       "1                   Amazon             Technology  \n",
       "2                     LVMH       Fashion & Retail  \n",
       "3                Microsoft             Technology  \n",
       "4       Berkshire Hathaway  Finance & Investments  \n",
       "..                     ...                    ...  \n",
       "195                   beer        Food & Beverage  \n",
       "196         private equity  Finance & Investments  \n",
       "197                 Airbnb             Technology  \n",
       "198  movies, record labels  Media & Entertainment  \n",
       "199         semiconductors          Manufacturing  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.forbes.com/?sh=1242c2bf2254\")\n",
    "time.sleep(3)\n",
    "\n",
    "#let's get option button from the page\n",
    "opt_btn = driver.find_element(By.XPATH, \"//div[@class='header__left']//button\")\n",
    "opt_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#select billionaires from options\n",
    "blns = driver.find_element(By.XPATH,\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "blns.click()\n",
    "time.sleep(3)\n",
    "#select world billionaire\n",
    "bln_list = driver.find_element(By.XPATH,\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "bln_list.click()\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# creating empty lists for storing scrapped data\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "Net_worth = []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # scraping rank of the billionaires\n",
    "    rank_tag = driver.find_elements(By.XPATH,\"//div[@class='rank']\")\n",
    "    for rank in rank_tag:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    " \n",
    "    # scraping the names of the billionaires\n",
    "    name_tag = driver.find_elements(By.XPATH,\"//div[@class='personName']/div\")\n",
    "    for name in name_tag:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the age of the billionaires\n",
    "    age_tag = driver.find_elements(By.XPATH,\"//div[@class='age']/div\")\n",
    "    for age in age_tag:\n",
    "        Age.append(age.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # scraping the  citizenship of the billionaires\n",
    "    cit_tag = driver.find_elements(By.XPATH,\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tag:\n",
    "        Citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the  source of income of the billionaires\n",
    "    sour_tag = driver.find_elements(By.XPATH,\"//div[@class='source']\")\n",
    "    for sour in sour_tag:\n",
    "        Source.append(sour.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping industry of the billionaires\n",
    "    ind_tag = driver.find_elements(By.XPATH,\"//div[@class='category']//div\")\n",
    "    for ind in ind_tag:\n",
    "        Industry.append(ind.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping  net_worth of billionaires\n",
    "    net_tag = driver.find_elements(By.XPATH,\"//div[@class='netWorth']/div\")\n",
    "    for net in net_tag:\n",
    "        Net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "print(len(Rank),len(Person_Name),len(Net_worth),len(Age),len(Citizenship),len(Source),len(Industry))\n",
    "\n",
    "# converting into data frame\n",
    "All_Billionaires = pd.DataFrame({})\n",
    "All_Billionaires['Rank'] = Rank\n",
    "All_Billionaires['Name'] = Person_Name\n",
    "All_Billionaires['Net Worth'] = Net_worth\n",
    "All_Billionaires['Age'] = Age\n",
    "All_Billionaires['Citizenship'] = Citizenship\n",
    "All_Billionaires['Source'] = Source\n",
    "All_Billionaires['Industry'] = Industry\n",
    "All_Billionaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7a727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82939b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 540 1080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Comment Time</th>\n",
       "      <th>Comment Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>நாங்க யாருனு காட்ட வேண்டிய நேரம் இது Kollywood...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>44K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie has the best interval scene in Tami...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>1.1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Masterpiece movie!!\\nWhy is this masterpiece n...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>2.1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After watching movie 2 times... Trailer still ...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>പടം കണ്ട് കഴിഞ്ഞ് വന്നു ട്രൈലെർ വീണ്ടും കാണുന്...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Marana waiting to see lokesh Kanagaraj screenp...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>After a long time post k s ravikumar Kamal has...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Trailer mass, songs mass,movie pakka mass</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>சிங்கப்பூரில்பார்த்த முதல் படம் விக்ரம்</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Action ...with music  blaster</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comment  Comment Time  \\\n",
       "0    நாங்க யாருனு காட்ட வேண்டிய நேரம் இது Kollywood...  2 months ago   \n",
       "1    This movie has the best interval scene in Tami...   1 month ago   \n",
       "2    Masterpiece movie!!\\nWhy is this masterpiece n...   1 month ago   \n",
       "3    After watching movie 2 times... Trailer still ...  2 months ago   \n",
       "4    പടം കണ്ട് കഴിഞ്ഞ് വന്നു ട്രൈലെർ വീണ്ടും കാണുന്...   1 month ago   \n",
       "..                                                 ...           ...   \n",
       "495  Marana waiting to see lokesh Kanagaraj screenp...  2 months ago   \n",
       "496  After a long time post k s ravikumar Kamal has...  2 months ago   \n",
       "497          Trailer mass, songs mass,movie pakka mass  2 months ago   \n",
       "498            சிங்கப்பூரில்பார்த்த முதல் படம் விக்ரம்  2 months ago   \n",
       "499                     Action ...with music  blaster   2 months ago   \n",
       "\n",
       "    Comment Upvotes  \n",
       "0               44K  \n",
       "1              1.1K  \n",
       "2              2.1K  \n",
       "3               535  \n",
       "4               217  \n",
       "..              ...  \n",
       "495             217  \n",
       "496               5  \n",
       "497                  \n",
       "498               1  \n",
       "499                  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.youtube.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# finding the web element for search bar and writing on the search bar\n",
    "search_bar = driver.find_element(By.XPATH,\"//div[@class='ytd-searchbox-spt']/input\")\n",
    "search_bar.send_keys(\"Vikram\")      \n",
    "time.sleep(2)\n",
    "\n",
    "#clicking on the search button\n",
    "search_btn = driver.find_element(By.ID,\"search-icon-legacy\")\n",
    "search_btn.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# clicking on first video\n",
    "video = driver.find_element(By.XPATH,\"//yt-formatted-string[@class='style-scope ytd-video-renderer']\")\n",
    "video.click()\n",
    "\n",
    "for _ in range(10000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")\n",
    "    \n",
    "# creating empty lists for storing the scrapped data\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "# scraping comments\n",
    "cm = driver.find_elements(By.ID,\"content-text\")\n",
    "for i in cm:\n",
    "    if i.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# scraping the  time when comment was posted\n",
    "tm = driver.find_elements(By.XPATH,\"//a[contains(text(),'ago')]\")\n",
    "for i in tm:\n",
    "    Time.append(i.text)\n",
    "    \n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# scraping the likes in comments\n",
    "like = driver.find_elements(By.XPATH,\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "\n",
    "time.sleep(5)\n",
    "#checking the length of lists\n",
    "print(len(comments),len(comment_time),len(No_of_Likes))\n",
    "\n",
    "# convertingt he data into a dataframe\n",
    "\n",
    "Youtube = pd.DataFrame({})\n",
    "Youtube['Comment'] = comments[:500]\n",
    "Youtube['Comment Time'] = comment_time[:500]\n",
    "Youtube['Comment Upvotes'] = No_of_Likes[:500]\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936dadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# locating the web element for location search bar\n",
    "search_bar = driver.find_element(By.XPATH,\"//*[@id='search-input-field']\")\n",
    "\n",
    "# entering desired location in search bar\n",
    "search_bar.send_keys(\"London\")\n",
    "\n",
    "# select London\n",
    "London = driver.find_element(By.XPATH,\"//ul[@id='predicted-search-results']//li[2]\")\n",
    "\n",
    "#clicking on button\n",
    "London.click()\n",
    "\n",
    "# Clicking on the search button\n",
    "\n",
    "search_btn = driver.find_element(By.ID,'search-button')\n",
    "search_btn.click()\n",
    "\n",
    "# creating empty list for storing scrapped data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description = []\n",
    "url = []\n",
    "\n",
    "# scraping the required informations\n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping  hostel name\n",
    "    try:\n",
    "        name = driver.find_elements(By.XPATH,\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "        \n",
    "    # scraping distance from city centre\n",
    "    try:\n",
    "        dist = driver.find_elements(By.XPATH,\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in name:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "   \n",
    "    for i in driver.find_elements(By.XPATH,\"//div[@class='prices-col']\"):   \n",
    "    # scraping privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element(By.XPATH,\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "            \n",
    "    for i in driver.find_elements(By.XPATH,\"//div[@class='prices-col']\"):          \n",
    "    # scraping dorms from price\n",
    "        try:\n",
    "            dorms = driver.find_element(By.XPATH,\"//a[@class='prices']//div[2]/div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            \n",
    "            \n",
    "    # scraping facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements(By.XPATH,\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements(By.XPATH,\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text)\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "     \n",
    "            \n",
    "    #fetching url of each hostel\n",
    "    p_url = driver.find_elements(By.XPATH,\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        url.append(i.get_attribute(\"href\"))\n",
    "        \n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "\n",
    "    # scraping ratings\n",
    "    try:\n",
    "        rat = driver.find_element(By.XPATH,\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "     # scraping total review\n",
    "    try:\n",
    "        rws = driver.find_element(By.XPATH,\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        \n",
    "        \n",
    "    # fetching over all review\n",
    "    try:\n",
    "        overall = driver.find_element(By.XPATH,\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "        \n",
    "        \n",
    "    # fetching property description\n",
    "    try:\n",
    "        disc = driver.find_element(By.XPATH,\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    \n",
    "    #  clicking on show more button for description\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(4)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "\n",
    "print(len(hostel_name),len(distance),len(pvt_prices),len(dorms_price),len(rating),len(reviews),len(over_all),len(facilities),len(description),len(url))\n",
    "\n",
    "# converting the scrapped data into data frame\n",
    "Hostel = pd.DataFrame({})\n",
    "Hostel['Hostel Name'] = hostel_name\n",
    "Hostel['Distance from City Centre'] = distance\n",
    "Hostel['Ratings'] = rating\n",
    "Hostel['Total Reviews'] = reviews\n",
    "Hostel['Overall Reviews'] = over_all\n",
    "Hostel['Privates from Price'] = pvt_prices\n",
    "Hostel['Dorms from Price'] = dorms_price\n",
    "Hostel['Facilities'] = facilities[:74]\n",
    "Hostel['Description'] = description\n",
    "Hostel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714329ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d62b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec17637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303f366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
