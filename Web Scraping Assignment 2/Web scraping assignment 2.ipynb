{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acddf475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\sanka\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from selenium) (1.26.10)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (37.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (22.0.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\sanka\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27531653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr.Business Data Analyst</td>\n",
       "      <td>Collabera</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, karnataka\\n(WFH during Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job opportunity For Data Analyst at Trellance ...</td>\n",
       "      <td>CURise Analytics Pvt. Ltd.</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Ahmedabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring For Data Analyst</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Optum</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Hudsons bay Company (HBC)</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>KrazyBee</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru(Old Madras Road)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Master Data Management Business Analyst</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Job Titles  \\\n",
       "0                           Sr.Business Data Analyst   \n",
       "1  Job opportunity For Data Analyst at Trellance ...   \n",
       "2      Data Analyst - Python/Artificial Intelligence   \n",
       "3                            Hiring For Data Analyst   \n",
       "4                                       Data Analyst   \n",
       "5                             Associate Data Analyst   \n",
       "6                             Associate Data Analyst   \n",
       "7                                Senior Data Analyst   \n",
       "8                                Senior Data Analyst   \n",
       "9            Master Data Management Business Analyst   \n",
       "\n",
       "                      Company Experience  \\\n",
       "0                   Collabera   6-11 Yrs   \n",
       "1  CURise Analytics Pvt. Ltd.    0-2 Yrs   \n",
       "2           iMindYourBusiness    0-2 Yrs   \n",
       "3                    Flipkart    2-5 Yrs   \n",
       "4                       Wipro    4-9 Yrs   \n",
       "5                       Optum    2-7 Yrs   \n",
       "6                       Optum    1-4 Yrs   \n",
       "7   Hudsons bay Company (HBC)    3-4 Yrs   \n",
       "8                    KrazyBee    3-6 Yrs   \n",
       "9                   Accenture    6-8 Yrs   \n",
       "\n",
       "                                            Location  \n",
       "0  Bangalore/Bengaluru, karnataka\\n(WFH during Co...  \n",
       "1                     Bangalore/Bengaluru, Ahmedabad  \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...  \n",
       "3                                Bangalore/Bengaluru  \n",
       "4  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...  \n",
       "5                                Bangalore/Bengaluru  \n",
       "6                                Bangalore/Bengaluru  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8               Bangalore/Bengaluru(Old Madras Road)  \n",
       "9                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#finding web elemet for search-job bar and writing on search-job bar \n",
    "search_job=driver.find_element_by_class_name('suggestor-input')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "#finding web elemet for search-location bar and writing on search-location bar \n",
    "search_locn=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_locn.send_keys(\"Bangalore\")\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn.click()\n",
    "\n",
    "#extracting job_titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "    \n",
    "\n",
    "#scraping company name\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "\n",
    "company_names=[]\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "\n",
    "\n",
    "#scraping experience required\n",
    "exp_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "\n",
    "\n",
    "experience=[]\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "    \n",
    "#scraping job Location\n",
    "locn_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "\n",
    "\n",
    "location=[]\n",
    "for i in locn_tags:\n",
    "    location.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "#converting all the lists into dataframe\n",
    "len(job_titles),len(company_names),len(experience),len(location)\n",
    "\n",
    "jobs=pd.DataFrame()\n",
    "jobs['Job Titles']=job_titles\n",
    "jobs['Company']=company_names\n",
    "jobs['Experience']=experience\n",
    "jobs['Location']=location\n",
    "\n",
    "jobs[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a124f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddfa6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring For Senior Data Scientist</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data &amp; Analytics Tech - Informatica Cloud- Sen...</td>\n",
       "      <td>PwC</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataiku Consultant</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Research and Development -AI/ML -(PhD )</td>\n",
       "      <td>EXL</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opportunity For Data Scientist - Female Candid...</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Science Engineer</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science - Engineering Manager</td>\n",
       "      <td>Paytm</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EY GBS - Assistant Director - Data Science (10...</td>\n",
       "      <td>EYGBS</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Job Titles  \\\n",
       "0  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "1                   Hiring For Senior Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3  Data & Analytics Tech - Informatica Cloud- Sen...   \n",
       "4                                 Dataiku Consultant   \n",
       "5            Research and Development -AI/ML -(PhD )   \n",
       "6  Opportunity For Data Scientist - Female Candid...   \n",
       "7                       Senior Data Science Engineer   \n",
       "8                 Data Science - Engineering Manager   \n",
       "9  EY GBS - Assistant Director - Data Science (10...   \n",
       "\n",
       "                           Company  \\\n",
       "0                            Wipro   \n",
       "1  TATA CONSULTANCY SERVICES (TCS)   \n",
       "2                Applied Materials   \n",
       "3                              PwC   \n",
       "4                            Wipro   \n",
       "5                              EXL   \n",
       "6                             PayU   \n",
       "7                Fractal Analytics   \n",
       "8                            Paytm   \n",
       "9                            EYGBS   \n",
       "\n",
       "                                            Location  \n",
       "0  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...  \n",
       "1                          Bangalore/Bengaluru, Pune  \n",
       "2                                Bangalore/Bengaluru  \n",
       "3                                Bangalore/Bengaluru  \n",
       "4                 Bangalore/Bengaluru, Pune, Chennai  \n",
       "5  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...  \n",
       "6  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...  \n",
       "7  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...  \n",
       "8                 Bangalore/Bengaluru, Noida, Mumbai  \n",
       "9                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#finding web elemet for search-job bar and writing on search-job bar \n",
    "search_job=driver.find_element_by_class_name('suggestor-input')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "#finding web elemet for search-location bar and writing on search-location bar \n",
    "search_locn=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_locn.send_keys(\"Bangalore\")\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn.click()\n",
    "\n",
    "#extracting job_titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "    \n",
    "\n",
    "#scraping company name\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "\n",
    "company_names=[]\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "\n",
    "\n",
    "#scraping job Location\n",
    "locn_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "\n",
    "\n",
    "location=[]\n",
    "for i in locn_tags:\n",
    "    location.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "#converting all the lists into dataframe\n",
    "len(job_titles),len(company_names),len(location)\n",
    "\n",
    "jobs=pd.DataFrame()\n",
    "jobs['Job Titles']=job_titles\n",
    "jobs['Company']=company_names\n",
    "jobs['Location']=location\n",
    "\n",
    "jobs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c187d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5957e302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>EXL</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Associate - Data Science</td>\n",
       "      <td>Black Turtle</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine learning AI</td>\n",
       "      <td>Teq Analytics</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>Noida</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science Associate</td>\n",
       "      <td>Kreate Energy</td>\n",
       "      <td>Delhi / NCR(Vaishali)</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Job Titles  \\\n",
       "0             DigitalBCG GAMMA Data Scientist   \n",
       "1            Data Scientist - Noida/Bangalore   \n",
       "2             Senior Associate - Data Science   \n",
       "3        Data Scientist - Machine learning AI   \n",
       "4  Data Scientist For Healthcare Product team   \n",
       "5  Data Scientist For Healthcare Product team   \n",
       "6              Data Scientist - MIND Infotech   \n",
       "7           Data Scientist - Engine Algorithm   \n",
       "8                      Data Science Associate   \n",
       "9                              Data Scientist   \n",
       "\n",
       "                                    Company  \\\n",
       "0                   Boston Consulting Group   \n",
       "1                                       EXL   \n",
       "2                              Black Turtle   \n",
       "3                             Teq Analytics   \n",
       "4                  SECUREKLOUD TECHNOLOGIES   \n",
       "5                  SECUREKLOUD TECHNOLOGIES   \n",
       "6  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED   \n",
       "7                              Primo Hiring   \n",
       "8                             Kreate Energy   \n",
       "9   Mount Talent Consulting Private Limited   \n",
       "\n",
       "                                            Location Experience  \n",
       "0                     New Delhi, Bangalore/Bengaluru    2-5 Yrs  \n",
       "1                         Noida, Bangalore/Bengaluru   5-10 Yrs  \n",
       "2  Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...    4-7 Yrs  \n",
       "3  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-8 Yrs  \n",
       "4          Delhi / NCR, Chennai, Bangalore/Bengaluru    2-7 Yrs  \n",
       "5          Delhi / NCR, Chennai, Bangalore/Bengaluru    2-7 Yrs  \n",
       "6                                              Noida    4-8 Yrs  \n",
       "7  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...    1-3 Yrs  \n",
       "8                              Delhi / NCR(Vaishali)    2-4 Yrs  \n",
       "9             Delhi / NCR, Pune, Bangalore/Bengaluru    2-4 Yrs  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#finding web elemet for search-job bar and writing on search-job bar \n",
    "search_job=driver.find_element_by_class_name('suggestor-input')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn.click()\n",
    "\n",
    "#Clicking on the salary and location check boxes\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "salary_check=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]')\n",
    "salary_check.click()\n",
    "time.sleep(10)\n",
    "\n",
    "locn_check=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[3]/label/p')\n",
    "locn_check.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#extracting job_titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "    \n",
    "\n",
    "#scraping company name\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "\n",
    "company_names=[]\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "\n",
    "\n",
    "#scraping job Location\n",
    "locn_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "\n",
    "\n",
    "location=[]\n",
    "for i in locn_tags:\n",
    "    location.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "#scraping experience required\n",
    "exp_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "\n",
    "\n",
    "experience=[]\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "#converting all the lists into dataframe\n",
    "len(job_titles),len(company_names),len(location), len(experience)\n",
    "\n",
    "jobs=pd.DataFrame()\n",
    "jobs['Job Titles']=job_titles\n",
    "jobs['Company']=company_names\n",
    "jobs['Location']=location\n",
    "jobs['Experience']=experience\n",
    "jobs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce06586",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9436fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (52)</td>\n",
       "      <td>₹649</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAHAAZIL</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Wa...</td>\n",
       "      <td>₹177</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹298</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹264</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Roadster</td>\n",
       "      <td>Others Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹649</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹246</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹154</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart UV Protection Rectangular Sunglass...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand Name                                        Description Price  \\\n",
       "0    VINCENT CHASE          UV Protection Rectangular Sunglasses (52)  ₹649   \n",
       "1   ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...  ₹474   \n",
       "2         DAHAAZIL  UV Protection, Night Vision, Riding Glasses Wa...  ₹177   \n",
       "3        Elligator                UV Protection Round Sunglasses (54)  ₹298   \n",
       "4        New Specs   UV Protection Rectangular Sunglasses (Free Size)  ₹264   \n",
       "..             ...                                                ...   ...   \n",
       "95        Roadster              Others Aviator Sunglasses (Free Size)  ₹649   \n",
       "96          PIRASO              UV Protection Aviator Sunglasses (54)  ₹246   \n",
       "97        Fastrack       UV Protection Aviator Sunglasses (Free Size)  ₹639   \n",
       "98       New Specs   UV Protection Rectangular Sunglasses (Free Size)  ₹154   \n",
       "99   VINCENT CHASE  by Lenskart UV Protection Rectangular Sunglass...  ₹749   \n",
       "\n",
       "   Discount  \n",
       "0   67% off  \n",
       "1   76% off  \n",
       "2   82% off  \n",
       "3   88% off  \n",
       "4   89% off  \n",
       "..      ...  \n",
       "95  35% off  \n",
       "96  84% off  \n",
       "97  20% off  \n",
       "98  89% off  \n",
       "99  62% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "#Bypassing throught the login details\n",
    "cross_btn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cross_btn.click()\n",
    "\n",
    "#finding web elemet for search bar and writing on search bar \n",
    "search_bar=driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_bar.send_keys(\"Sunglass\")\n",
    "\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#extracting brand name , discount, price and description of the sunglasses\n",
    "brand_name=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    disc_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for i in brand_tags:\n",
    "        brand_name.append(i.text)\n",
    "    for i in desc_tags:\n",
    "        description.append(i.text)\n",
    "    for i in price_tags:\n",
    "        price.append(i.text)\n",
    "    for i in disc_tags:\n",
    "        discount.append(i.text)\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "next_button = driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n",
    "\n",
    "\n",
    "#converting all the lists into dataframe\n",
    "\n",
    "\n",
    "info=pd.DataFrame()\n",
    "info['Brand Name']=brand_name\n",
    "info['Description']=description\n",
    "info['Price']=price\n",
    "info['Discount']=discount\n",
    "info[0:100]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2925469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "#Bypassing throught the login details\n",
    "cross_btn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cross_btn.click()\n",
    "\n",
    "#finding web elemet for search bar and writing on search bar \n",
    "search_bar=driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_bar.send_keys(\"Iphone 11\")\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(5)\n",
    "#clicking on Iphone 11 in flipcart\n",
    "iphone_11=driver.find_element_by_xpath(\"//img[@class='_396cs4 _3exPp9']\")\n",
    "iphone_11.click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "#extracting rating star\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "\n",
    "\n",
    "ratings=[]\n",
    "for i in rating_tags:\n",
    "    ratings.append(i)\n",
    "    \n",
    "time.sleep(10)\n",
    "\n",
    "#scraping rating text\n",
    "text_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "\n",
    "\n",
    "text=[]\n",
    "for i in text_tags:\n",
    "    text.append(i.text)\n",
    "    \n",
    "\n",
    "time.sleep(10)   \n",
    "\n",
    "#scraping review\n",
    "review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "\n",
    "\n",
    "review=[]\n",
    "for i in review_tags:\n",
    "    review.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "#converting all the lists into dataframe\n",
    "\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['Star Rating']=ratings\n",
    "df['Rating Text']=text\n",
    "df['Review']=review\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db456e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6.Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "#Bypassing throught the login details\n",
    "cross_btn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cross_btn.click()\n",
    "\n",
    "#finding web elemet for search bar and writing on search bar \n",
    "search_bar=driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_bar.send_keys(\"sneakers\")\n",
    "\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#extracting brand name , discount, price and description of the sunglasses\n",
    "brand_name=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    disc_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for i in brand_tags:\n",
    "        brand_name.append(i.text)\n",
    "    for i in desc_tags:\n",
    "        description.append(i.text)\n",
    "    for i in price_tags:\n",
    "        price.append(i.text)\n",
    "    for i in disc_tags:\n",
    "        discount.append(i.text)\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "next_button = driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n",
    "\n",
    "\n",
    "#converting all the lists into dataframe\n",
    "\n",
    "\n",
    "info=pd.DataFrame()\n",
    "info['Brand Name']=brand_name\n",
    "info['Description']=description\n",
    "info['Price']=price\n",
    "info['Discount']=discount\n",
    "info[0:100]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7.Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ee044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.myntra.com/shoes\")\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "price_check=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label')\n",
    "price_check.click()\n",
    "time.sleep(10)\n",
    "\n",
    "color_check=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]')\n",
    "color_check.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#extracting brand name, description, price of the shoes\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "    price_tags=driver.find_elements_by_xpath(\"//span[@class='product-discountedPrice']\")\n",
    "    \n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text)\n",
    "    for i in desc_tags:\n",
    "        description.append(i.text)\n",
    "    for i in price_tags:\n",
    "        price.append(i.text)\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "next_button = driver.find_element_by_xpath(\"//li[@class='pagination-next']\")\n",
    "next_button.click()\n",
    "   \n",
    "#converting all the lists into dataframe\n",
    "len(brand),len(description),len(price)\n",
    "\n",
    "shoes=pd.DataFrame()\n",
    "shoes['Brand']=brand\n",
    "shoes['Description']=description\n",
    "shoes['Price']=price\n",
    "\n",
    "shoes[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8.Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7992b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\sanka\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe')\n",
    "\n",
    "#connecting to url of our requirement\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "#finding web elemet for search-job bar and writing on search-job bar \n",
    "search_bar=driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_bar.send_keys(\"Laptop\")\n",
    "\n",
    "\n",
    "#finding the web element for search button and clicking it\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search_btn.click()\n",
    "\n",
    "\n",
    "\n",
    "#extracting laptop titles\n",
    "brand_tags=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "\n",
    "\n",
    "brand=[]\n",
    "for i in brand_tags:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "\n",
    "#scraping laptop rating\n",
    "rating_tags=driver.find_elements_by_xpath(\"//span[@class='a-declarative']\")\n",
    "\n",
    "\n",
    "rating=[]\n",
    "for i in rating_tags:\n",
    "    rating.append(i.text)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "#scraping price\n",
    "price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price']\")\n",
    "\n",
    "\n",
    "price=[]\n",
    "for i in price_tags:\n",
    "    price.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "#converting all the lists into dataframe\n",
    "\n",
    "laptops=pd.DataFrame()\n",
    "laptops['Title']=brand\n",
    "laptops['Rating']=rating\n",
    "laptops['Price']=price\n",
    "\n",
    "laptops[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9.Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "ASSIGNMENT 2\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca096038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39136c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef5d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682d854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44cca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68aac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75027e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9937a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389c8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a3292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142e549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd8e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c4f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec5d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
